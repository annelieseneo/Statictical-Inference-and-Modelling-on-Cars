\documentclass{article}
@ load packages
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lscape}
\usepackage{setspace}
\usepackage{scrextend}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{color}
\usepackage{soul}
\PassOptionsToPackage{hyphens}{url}

@ cover page 
\title{\normalsize{ITS66804 Statistical Inference and Modelling\\Lecturer : Dr Thulasyammal Ramiah Pillai\\.}\\ {\LARGE\bfseries Final Assessment}}
\author{by\\ \\Neo Ann Yi (0340281)}
\date{10th December 2020}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle{}

\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
@ table of contents 
\tableofcontents
\newpage

@ contents
@ introduction 
\section{ChickWeight Dataset}
The ChickWeight dataset focuses on the topic of weight progression of chicks that were assigned a specific experimental protein diet starting from the early growth stages. There are four such diet types, and farmers are interested to find the best diet that is fastest in fattening the chicks. This allows the chicks to grow and reach the acceptable market carcass weight of 1850 grams at a sooner time, which can help to speed up revenue generation (Hamre, 2018).\\
Data on the chick's weight and age were collected and recorded, as measured from day 1 of birth and then every other day until day 21.\\
The objective is to analyse the available data, and design and develop a predictive model to cater to the farmers for identifying the best diet.\\
The aim is to emphasize the importance of selecting an effective diet, through the comparison of their differences for the daily increase in weights.

@ load data
\subsection{Data}
The relevant and reliable built-in dataset is loaded. A sample of the first five records are shown below. 
\begin{center}
\begin{minipage}{0.3\textwidth} 
<<echo = FALSE>>=
library(datasets) #load the library
dataset = ChickWeight # load dataset
head(dataset, 5) #load the first five records in dataset
@
\end{minipage} 
\\Figure 1: Sample of Dataset 
\end{center}

\noindent The dimensions of the dataset, in other words the number of records/rows and attributes/columns, is computed below.
<<>>=
dim(dataset) # dimensions of dataset
@

\noindent The dataset is found to have 4 attributes and 578 records.\\
The 'weight' attribute is the body weight of the chick in grams, as measured at day 1 of birth and then every other day until day 21. 'Time' is the age in number of days since birth, in alternate days from day 1 to day 21. 'Chick' is an ordered unique identifier for each chick. 'Diet' represents the four experimental protein diets.

\subsection{Noises and Errors}
Noises are duplicated records, impossible or extreme values, and outliers. \\
Errors are inconsistent and obvious redundant attributes. \\
These are all checked as follows.

<<>>=
library(plyr)
count(duplicated(dataset) == TRUE) #identify duplicated values
count(is.na(dataset) == TRUE) #identify missing values
summary(dataset) #identify impossible or extreme values

#identify outliers
boxplot.stats(dataset$weight)$out 
boxplot.stats(dataset$Time)$out
@

\noindent For this case study, only noises in terms of outliers are identified for the attribute 'weight'. These outlier values are 318, 331, 327, 341, 332, 361, 373, 321, and 322.\\
The Interquartile Range (IQR) method is used to detect these outlier values as those beyond the range of [1.5 x IQR - Qtl1, 1.5 x IQR + Qtl3] in which Qtl1 represents the first quartile or the 25th percentile and Qtl3 represents the third quartile or the 75th percentile. The Winsorisation formula is used to smooth these outlier values to the maximum of minimum allowed value that will not be considered as outliers. In this case, the maximum allowed value for 'weight' is 309, and the minimum is 35.
<<echo=FALSE>>=
# smooth outliers using the winsorisation formula and the interquartile range (IQR) method
dataset$weight <- ifelse(dataset$weight < 35, 35, dataset$weight)
dataset$weight <- ifelse(dataset$weight  > 309, 309, dataset$weight)
@

\noindent The 'weight' data is checked again to ensure that the outliers have been successfully smoothed.
<<>>=
boxplot.stats(dataset$weight)$out #identify outliers
@

@ eda
\subsection{Exploratory Data Analysis}
\subsubsection{Descriptive Statistics}
<<>>=
class(dataset) # class of dataset is dataframe
typeof(dataset) # type of dataset is list
sapply(dataset, class) # types of attributes
dim(dataset) # dimension of dataset
# change to numerical format for descriptive statistics
dataset$Chick = as.numeric(dataset$Chick)
dataset$Diet = as.numeric(dataset$Diet)
sapply(dataset, class) # new types of attributes
@

\noindent Exploratory data analysis (EDA) is helpful in performing initial investigations on data before formal modeling and graphical representations, in order to discover patterns, look over assumptions, and test hypothesis. The dataset class is a dataframe of grouped data. The type of dataset is a list. The attribute 'weight' is quantitative continuous data type, 'Time' is quantitative discrete integer data type, 'Chick' is quantitative discrete data type consisting of ordered factor levels, and 'Diet' is quantitative discrete categorical data type. \\
The dimension of the dataset is 578 rows/records and 4 attributes/columns.\\
In order to facilitate better descriptive statistical analysis, the attributes 'Chick' and 'Diet' are converted from factors to numerical data. Now, all 4 attributes have numerical data types.

\noindent 

\paragraph{Frequency Distribution Tables}
\vspace*{1\baselineskip}
The frequency distribution tables list the frequency of occurences of each attribute value. In other words, this visualises the frequency distribution and identifies preliminary patterns in data, especially the mode value. For instance, the mode value for 'weight' is 41 grams, and 'Diet' is 1.
<<>>=
# frequency distribution tables
count(dataset, 'weight')
count(dataset, 'Time')
count(dataset, 'Chick')
count(dataset, 'Diet')
@

\paragraph{Measures of Central Tendency, Measures of Dispersion, and Skewness}
\vspace*{1\baselineskip}
Summary statistics include measures of central tendency and measures of dispersion, which are useful in providing a quick and simple description of the dataset and its characteristics.\\
The measures of central tendency will identify the data's central position, which are the more common values. This can be measured using the average or also known as the arithmetic mean, or using the median or the mode.\\
The measures of dispersion or spread is the amount of variability in data instances. This can be measured using the standard deviation, the three quartiles, and the variance (Laerd statistics, n.d.).
\newpage
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccc}
Parameter & Weight (Grams) & Time (Age in Days) & Chick & Diet\\
\hline
Min & 35.0 & 0.0 & 1.0 & 1.0\\
Mean & 121.400 & 10.720 & 26.260 & 2.235\\
First Quartile & 63.0 & 4.0 & 14.0 & 1.0\\
Second Quartile (Median) & 103.0 & 10.0 & 26.0 & 2.0\\
Third Quartile & 163.8 & 16.0 & 38.0 & 3.0\\
Mode & 41.0 & 0.0 & 15.0 & 1.0\\
Max & 309.0 & 21.0 & 50.0 & 4.0\\
Variance & 4875.5252 & 45.6760 & 195.9117 & 1.3518\\
Standard Deviation & 69.82500 & 6.7584 & 13.9968 & 1.1627\\
Coefficient of Variance (CV) & 0.5835 & 0.6304 & 0.5330 & 0.5202\\
Skewness & 0.8710 & -0.0175 & -0.0015 & 0.3155\\
Kurtosis & 2.9718 & 1.7427 & 1.8123 & 1.6132\\
\end{tabular}
\end{center}
\caption{Summary Statistics}
\label{table:stats1}
\end{table}

\noindent The coefficient of variance (CV) is the relative standard deviation that standardises the measure of dispersion for frequency distributions. The CV of all 4 attribute are smaller than 1, which means that all their data points are approximately centered, with good variability and dispersion compared to their means.\\
On top to that, the skewness and kurtosis is helpful in describing the data distribution. 'Time', 'Chick', and 'Diet' have skew values between -0.5 and 0.5, so their data is approximately normally distributed due to a slight asymmetry. 'weight' is moderately skewed with skew values between the absolute values of 0.5 and 1.\\
In addition to the skewness of data distribution, the weight in the distribution's left or right tail can also be identified by the negative and positive skew value respectively (Kaufmann, 2014). 'weight' and 'Diet' have more weight in the distribution's right tail, while the other two remaining predictors' data distributions have more weight in the distribution's left tail. Low skewed and kurtosis will indicate a light-tailed distribution relative to a normal distribution, with little to no presence extreme values.\\
For instance, the attribute 'weight' has a mean of 121.4, median of 103, and mode of 41 are relatively unequal, and in addition to its CV of 0.5835 and skew value of -0.8710, it is suggested to have a moderately asymmetric and relatively heavy right-tailed data distribution (Kaufmann, 2014).\\
The kernel density estimate (KDE) plots, as visualised in Section 1.3.2 Preliminary Visualisations and Mathematical Modelling, are additionally used to provide a more complete picture of these conclusions.

\paragraph{Correlations}
<<>>=
cor(dataset) # correlation matrix
@
\noindent A correlation matrix is a tool to describe the measures of association between attributes. It lists all the Pearson product-moment correlation coefficients. The correlations will compare and describe the linear connection and relationship between pairs of attributes, through the type of correlation and its strength. A positive correlation means that both attribute' values will change in the exact same direction, while a negative correlation means that both attribute' values will change in opposite directions. The larger the correlation strength, the stronger the connection and relationship between that pair of attributes.\\
For instance, it is found that the 'weight' and 'Time' predictors have the most notable linear relationship with a high correlation coefficient of 0.8433.

\subsubsection{Preliminary Visualisations and Mathematical Modelling}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
<<echo = FALSE, fig = TRUE>>= 
# plot pie chart of target attribute
d1 <- sum(dataset['Diet'] == 1)
d2 <- sum(dataset['Diet'] == 2)
d3 <- sum(dataset['Diet'] == 3)
d4 <- sum(dataset['Diet'] == 4)
slices <- c(d1, d2, d3, d4) 
lbls <- c("Diet 1", "Diet 2", "Diet 3", "Diet 4") # labels
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) 
lbls <- paste(lbls,"%",sep="") # append % to labels 
colors <- c("salmon","firebrick3", "red", "maroon") # colour of slices
pie(slices,labels = lbls, col=colors,
    main="Pie Chart of Diets") #title of pie chart
legend("topright", c("Diet 1", "Diet 2", "Diet 3", "Diet 4"), 
       cex = 0.8, fill = colors) # legend
@
\captionof{figure}{Pie Chart of Diets}
\label{fig:piechart}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
<<echo = FALSE, fig = TRUE>>= 
# boxplot
boxplot(dataset[, 1:2], outline=FALSE, 
        main = "Boxplots of Time (Age in Days) and Weight (Grams)", # title of boxplot
        outwex=0.2, boxwex=0.5, col="mistyrose")
@
\captionof{figure}{Box Plot}
\label{fig:boxplot}
\end{subfigure}
\captionof{figure}{Pie Chart of Diets, and Boxplots of Age in Days and Weight in Grams}
\label{fig:piebox}
\end{figure}

\noindent  The counterplot pie chart clearly illustrates that data available for the 'Diet' group of 1 has a significantly larger proportion of 17\% difference in data counts as compared to the other groups. This class imbalance will be noted and refered to before performing data visualisations and analysis (Jaitley, 2019).\\
The boxplots illustrate a rough probability distribution for the attributes. The x-axis lists the attributes, while the y-axis measures the data values.\\
For instance, the mean, median and mode of 'Time' are almost equal, and its boxplot has symmetrical halves from the first to the third quartile (Lane, n.d.). Thus, it has an approximately normal distribution.\\
This normal distribution may be assumed as the probability distribution for the random variables of 'Time'. Due to this, the Central Limit Theorem should be followed by all its data. However, this method only takes into account of the data distribution between the first and third quartile and does not properly record the specific values and details of the distribution results. Therefore, its use is only acceptable for basic summaries.\\
The capability of fitting Log-normal probability distribution to the 'weight' attribute provides a great improvement to this method, as discussed below.\\
\\

\noindent Pair plots are visualised for the numerical data, and these are also known as scatterplot matrix. They are used to identify attributes with strong class-attribute relationship, since this is of most use and interest for this casy study purposes.\\
The upper triangle will list the strength of the linear relationships between two attributes, using Pearson's product-moment correlation coefficient as discussed above. Kernel density estimate (KDE) plots will illustrate the univariate distribution of a single attribute in relation to the target attribute in terms of a line graph. Histogram density plots are chosen to illustrate the overall data distribution, as well as the data distributions of diet types based on the predictor attributes. The scatter plots on the lower triangle will illustrate the distribution of data points. \\
More specifically on the KDE plots, it visualises the overall distribution through a continuous probability density curve for the Diet values in the same figure space, and clearly differentiates them by specifying different colours. The y-axis is the density measurements.\\
All the overall KDE distribution curves have at the most medium kurtosis and skewness, partly because the data instances available for each of the Diet groups have a significant difference. 'weight' has bimodal or multimodel data distribution that is asymmetric, since there are two distinct peaks or also known as local maxima. 'Time' has approximate normal data distribution as the curves are approximately symmetric, unimodal, asymptotic, and their mean, median, and mode are similar (Lane, n.d.). \\
The histogram density plots and their respective highest point in the curves show the patterns and trends in the numerical data. Specifically, Diet 1 has the lowest mean of 55 grams of weight, whereas Diet 3 has the highest of 70 grams. It is thus found that Diet 3 can achieve the highest increase in weight of chicks.\\
The 'Time' has significant overlapping or overplotting histogram density plots, and clearly shows that less useful meaning can be extracted. Its data points are more scattered and thus have more overlapping or overplotting areas, which means that they have little to no common relationship or pattern. It can thus be seen that 'weight' in grams is the better numerical predictor to differentiate Diet in this case.

\begin{figure}[H]
\begin{center}
<<echo = FALSE, fig = TRUE>>=
#install.packages("GGally")
library(GGally)
library(ggplot2)
# pair plot for continuous numerical data
ggpairs(dataset, columns = 1:4, 
        ggplot2::aes(colour=as.factor(dataset$Diet), alpha=9)) 
@
\captionof{figure}{Pair Plots}
\label{fig:pairplot}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
<<echo = FALSE>>= 
library("fitdistrplus")

# Fitting 5 probability distributions
gamma <- fitdist(dataset$weight, "gamma")
summary(gamma)
lnorm <- fitdist(dataset$weight, "lnorm")
summary(lnorm)
weibull <- fitdist(dataset$weight, "weibull")
summary(weibull)
norm <- fitdist(dataset$weight, "norm")
summary(norm)
exp <- fitdist(dataset$weight, "exp")
summary(exp)
@
\captionof{figure}{Mathematical Modelling through Fitting of Probability Distributions}
\label{fig:model}
\end{center}
\end{figure}

\noindent The best distribution is one with the best fit of curve and most accurately and efficiently predicts as the model. The distribution plot will be generated after fitting and comparing five distributions - Gamma, Normal, Weibull, Exponential, and Log-normal - to 'weight', and the comparison is done between their log-likelihoods, their corrected Akaike's Information Criterion (AIC), as well as their respective Bayesian Information Criterion (BIC)\\
The distribution with the largest log-likelihood should be chosen. It measures the likelihood's natural logarithm that will derive the maximum likelihood estimator, and indicates the best goodness-of-fit (Analyttica Datalab, 2019). \\
The smallest AICc and BIC should be chosen, since its equation formula will multiply -2 to its respective log likelihood value, and indicates least prediction error (Fabozzi et al., 2014). \\
\noindent Log-normal distribution is chosen, with log likelihood value of -3184.806, AIC of 6373.612, and BIC of 6382.331. It fits the distribution the best and is thus most accurate and efficient model for the data distribution of 'weight'. The distribution plot comprises of a histogram and the y-axis value plotting of a best fit curve of frequency density (McNeese, 2016). The standard error values will compute the confidence interval, in terms of minimal dispersion relative to the mean value (GraphPad Software, n.d.). 
\begin{figure}[H]
\begin{center}
<<echo = FALSE, fig = TRUE>>= 
# Visualization of probability distribution
denscomp(lnorm, addlegend = "FALSE",
         main="Histogram and Log-normal Distribution of Weight in Grams", #title of plot
         datacol = c("mistyrose"),
         fitcol = c("gray0"),
         fitlty= c("dashed"),
         xlab="Weight in Grams", #x axis label
         labels=TRUE, # data labels
         ylim=c(0,0.011), #y axis limit
         xlim=c(0,350)) #x axis limit
@
\captionof{figure}{Histogram and Log-normal Distribution of Weight}
\label{fig:hist}
\end{center}
\end{figure}
\noindent 

@ predictive modelling
\subsection{Predictive Modelling}
\noindent Four linear regression models, one for each diet type, will be generated. The best diet is decided as the one with the largest slope or gradient value in the equation of line of best fit, and the most accurate line. The line indicates a liear relationship best fitted between the dependent and independent attribute.\\
\noindent Weight is the dependent attribute on the y-axis, and Time in Age in Days is the independent attribute on the x-axis.

<<echo = FALSE>>=
# create new dataframes according to diet type
datasetDiet1 = subset(dataset, dataset['Diet'] == 1)
datasetDiet2 = subset(dataset, dataset['Diet'] == 2)
datasetDiet3 = subset(dataset, dataset['Diet'] == 3)
datasetDiet4 = subset(dataset, dataset['Diet'] == 4)
@

\begin{figure}[H]
\begin{center}
<<echo = FALSE, fig = TRUE>>=
# plot linear regression model
plot(weight ~ Time, data = datasetDiet1, xlab = "Time (age in days)", 
     ylab = "Weight (in grams)", # axis labels
     main = "Linear Regression Model of Time (Age in Days) and 
     Weight (Grams) for Diet 1", # title of plot
     pch = 20, cex = 2, col = "#F6BDC0")
abline(lm(weight ~ Time, data = datasetDiet1), lwd = 3, col = "#DC1C13") # plot line of best fit
@
\captionof{figure}{Linear Regression Model of Time (Age in Days) and Weight (Grams) for Diet 1}
\label{fig:modelLG}
\end{center}
\end{figure}

\noindent The summaries show each respective residuals, coefficients, standard error, rank of log-odds, and p-value. 
<<echo=FALSE>>=
# summary of linear regression model
model1 = lm(weight ~ Time, data = datasetDiet1)
summary(model1)
model2 = lm(weight ~ Time, data = datasetDiet2)
summary(model2)
model3 = lm(weight ~ Time, data = datasetDiet3)
summary(model3)
model4 = lm(weight ~ Time, data = datasetDiet4)
summary(model4)
@

\noindent The p-values are smaller than 0.05, thus rejecting the null hypothesis and the acceptance of the alternative that the models is significant and relevant in making estimations. The standard errors are low, thus suggesting high accuracy.\\
The models' outputs are best fit equations, where 'Time' can be substituted in to estimate the value of weight.\\
All of the values are similar to each other, and all diets can be considered as accurate, significant, and relevant. However, it can be clearly seen that Diet 3 has the largest coefficient among all. This means that Diet 3 can provide the largest daily increase in weight for the chicks. This is thus the best equation, which is weight = 20.3160 + 11.0802 x Time.\\
For instance, to predict the chick's weight when Diet 3 is assigned and the Time is 3 days, weight = 20.3160 + 11.0802 x 3 = 53.5566 grams.\\
The other three model equations are as follows:\\
For Diet 1 : weight = 30.9310 + 6.8418 x Time\\
For Diet 2 : weight = 28.9768 + 8.5540 x Time\\
For Diet 4 : weight = 30.9511 + 9.6893 x Time\\

@ model evaluation
\subsection{Model Performance Evaluations}
The difference between multiple R-squared and adjusted R-squared will measure the improvement other than expected of the model after adding a new number of predictors. The difference is large enough to be considered since only one predictor is added, which indicates that the model improved after adding the predictor of Time. The rank-3 log odds of Time in estimating weight shows a high association between it and weight, and the estimations are accurate with minimal standard error with minimal dispersion relative to the mean value. The p-value is lower than the 0.05 threshold, which indicates the rejection of the null hypothesis and the acceptance of the althernative that Time is significant, relevant, and important in estimating weight. Time is thus a good predictor of weight.\\
The confidence interval is computed below.\\
<<echo=TRUE>>=
# confidence interval of the model coefficient
confint(model1)
confint(model2)
confint(model3)
confint(model4)
@

\noindent All models are found to have high and wide confidence interval level in predicting weight. The entire data spread of the data is considered to be well-covered, with high confidence that it will contain the true mean value.

\noindent The error rates are computed below.\\
<<echo=TRUE>>=
# error rate
sigma(model1)/mean(datasetDiet1$Time)
sigma(model2)/mean(datasetDiet2$Time)
sigma(model3)/mean(datasetDiet3$Time)
sigma(model4)/mean(datasetDiet4$Time)
@

\noindent All models are found to have negligible error rates of around 3\%. This is comparatively good. However, even though Diet 3 has the best slope value as discussed above, the error rate of Diet 4 is relatively lower, which indicates that the predicted estimates are more accurate than that of Diet 3. Therefore, since Diet 3 and 4 only have a difference of 2 units of daily increase in weight, both Diets can be considered by the farmers.

\newpage
\section {mtcars Dataset}
The mtcars dataset is also known as the Motor Trend Car Road dataset. These records were collected by the 1974 Motor Trend US magazine, and they are extremely useful for the car industry production process to continuously improve in terms of cylinder usage, weights, and gas milage and fuel requirements, for better performance and more powerful cars. The performance is mainly measured in overall miles per gallon, while the other 10 attributes in the dataset will be performance and power-related factors.\\
The purpose is to develop a prediction model to emphasize the importance of specific performance and power related features in improving the overall performance of cars.

\subsection{Data}
\noindent The relevant and reliable built-in dataset is loaded. A sample of the first five records are shown below. 

\begin{center}
\begin{minipage}{0.8\textwidth} 
<<echo = FALSE>>=
library(datasets) #load the library
dataset2 = mtcars # load dataset
head(dataset2, 5) #load the first five records in dataset
@
\end{minipage} 
\\Figure 5: Sample of Dataset 
\end{center}

\noindent The dimensions of dataset are shown below.
<<>>=
dim(dataset2) # dimensions of dataset
@

\noindent The dataset is found to have 11 attributes and 32 records.\\
The 'mpg' attribute is the miles per gallon. 'cyl' is the number of cylinders. 'disp' is the displacement in cubic inches. 'hp' represents the horsepower. 'drat' is the rear axle ratio. 'wt' is the weight in thousands of pounds. 'qsec' is the quarter mile time. 'vs' is the V/S engine type. 'am' is the automatic/manual transmission. 'gear' is the number of forward gears. 'carb' is the number of carburetors.\\
In terms of their impact on performance or power, axel ratio affects speed and acceleration which in turn affects performance. Higher displacement is equivalent to more power. With higher power, qsec is shorter and thus faster to reach quarter mile. Lower weight reduce fuel consumption and thus improves performance.\\
Mpg is suitable to measure performance, while hp measures power.

\subsection{Noises and Errors}
Noises are duplicated records, impossible or extreme values, and outliers. \\
Errors are inconsistent and obvious redundant attributes. \\
These are all checked as follows.
<<>>=
library(plyr)
count(duplicated(dataset2) == TRUE) #identify duplicated values
count(is.na(dataset2) == TRUE) #identify missing values
summary(dataset2) #identify impossible or extreme values

#identify outliers
boxplot.stats(dataset2$mpg)$out 
boxplot.stats(dataset2$disp)$out
boxplot.stats(dataset2$hp)$out
boxplot.stats(dataset2$drat)$out
boxplot.stats(dataset2$wt)$out
boxplot.stats(dataset2$qsec)$out
@

\noindent For this case study, only noises in terms of outliers are identified for the attribute 'hp', 'wt', and 'qsec'. These outlier values are 335, 5.424, 5.345, and 22.9\\
The Interquartile Range (IQR) method is used to detect these outlier values as those beyond the range of [1.5 x IQR - Qtl1, 1.5 x IQR + Qtl3] in which Qtl1 represents the first quartile or the 25th percentile and Qtl3 represents the third quartile or the 75th percentile. The Winsorisation formula is used to smooth these outlier values to the maximum of minimum allowed value that will not be considered as outliers. In this case, the maximum allowed value for 'hp' is 264, and the minimum is 52, and so on for the other two attributes.
<<echo=FALSE>>=
# smooth outliers using the winsorisation formula and the interquartile range (IQR) method
dataset2$hp <- ifelse(dataset2$hp < 52, 52, dataset2$hp)
dataset2$hp <- ifelse(dataset2$hp > 264, 264, dataset2$hp)
dataset2$wt <- ifelse(dataset2$wt < 1.513, 1.513, dataset2$wt)
dataset2$wt <- ifelse(dataset2$wt > 5.25, 5.25, dataset2$wt)
dataset2$qsec <- ifelse(dataset2$qsec < 14.5, 14.5, dataset2$qsec)
dataset2$qsec <- ifelse(dataset2$qsec > 20.22, 20.22, dataset2$qsec)
@

\noindent The data are checked again to ensure that the outliers have been successfully smoothed.
<<>>=
boxplot.stats(dataset2$hp)$out #identify outliers
boxplot.stats(dataset2$wt)$out
boxplot.stats(dataset2$qsec)$out
@

\subsection{Exploratory Data Analysis}
\subsubsection{Descriptive Statistics}
<<>>=
class(dataset2) # class of dataset is dataframe
typeof(dataset2) # type of dataset is list
sapply(dataset2, class) # types of attributes
dim(dataset2) # dimension of dataset
@

\noindent Exploratory data analysis (EDA) is helpful in performing initial investigations on data before formal modeling and graphical representations, in order to discover patterns, look over assumptions, and test hypothesis. The dataset class is a dataframe of grouped data. The type of dataset is a list. The attribute 'cyl','vs','am','gear', and 'carb' have quantitative discrete integer or categorical data types, while the remaining 6 attributes have continuous data types. \\
The dimension of the dataset is 32 rows/records and 11 attributes/columns.\\

\paragraph{Measures of Central Tendency, Measures of Dispersion, and Skewness}
\noindent Summary statistics include measures of central tendency and measures of dispersion, which are useful in providing a quick and simple description of the dataset and its characteristics.\\
The measures of central tendency will identify the data's central position, which are the more common values. This can be measured using the average or also known as the arithmetic mean, or using the median or the mode.\\
The measures of dispersion or spread is the amount of variability in data instances. This can be measured using the standard deviation, the three quartiles, and the variance (Laerd statistics, n.d.).

\newpage
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccccc}
Parameter & mpg & cyl & disp & hp & drat & wt\\
\hline
Min & 10.400 & 4.000 & 71.100 & 52.000 & 2.760 & 1.513\\
Mean & 20.0900 & 6.1880 & 230.7000 & 144.5000 & 3.5970 & 3.2090\\
First Quartile & 15.430 & 4.000 & 120.800 & 96.500 & 3.080 & 2.581\\
Second Quartile (Median) & 19.200 & 6.000 & 196.300 & 123.000 & 3.695 & 3.325\\
Third Quartile & 22.800 & 8.000 & 326.000 & 180.000 & 3.920 & 3.610\\
Mode & 21.000 & 8.000 & 275.800 & 110.000 & 3.920 & 3.440\\
Max & 33.900 & 8.000 & 472.000 & 264.000 & 4.930 & 5.250\\
Variance & 36.3241 & 3.1895 & 15360.7998 & 3995.8054 & 0.2859 & 0.9208\\
Standard Deviation & 6.0269 & 1.7859 & 123.9387 & 63.2124 & 0.5347 & 0.9596\\
Coefficient of Variance (CV) & 0.3000 & 0.2886 & 0.5372 & 0.4375 & 0.1487 & 0.2990\\
Skewness & 0.6404 & -0.1831 & 0.4003 & 0.4216 & 0.2789 & 0.3644\\
Kurtosis & 2.7995 & 1.3190 & 1.9103 & 2.0493 & 2.4351 & 3.0471\\
\end{tabular}
\end{center}
\caption{Summary Statistics for First 6 Attributes}
\label{table:stats3}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccccc}
Parameter & qsec & vs & am & gear & carb\\
\hline
Min & 14.500 & 0.000 & 0.000 & 3.000 & 1.000\\
Mean & 17.7700 & 0.4375 & 0.4062 & 3.6880 & 2.8120\\
First Quartile & 16.890 & 0.000 & 0.000 & 3.000 & 1.000\\
Second Quartile (Median) & 17.710 & 0.000 & 0.000 & 4.000 & 2.000\\
Third Quartile & 18.900 & 1.000 & 1.000 & 4.000 & 4.000\\
Mode & 17.020 & 0.000 & 0.000 & 3.000 & 4.000\\
Max & 20.220 & 1.000 & 1.000 & 5.000 & 8.000\\
Variance & 2.5442 & 0.2540 & 0.2490 & 0.5444 & 2.6089\\
Standard Deviation & 1.5951 & 0.5040 & 0.4990 & 0.7378 & 1.6152\\
Coefficient of Variance (CV) & 0.0898 & 1.1520 & 1.2285 & 0.2001 & 0.5744\\
Skewness & -0.2288 & 0.2520 & 0.3818 & 0.5546 & 1.1021\\
Kurtosis & 2.3456 & 1.0635 & 1.1457 & 2.0568 & 4.5361\\
\end{tabular}
\end{center}
\caption{Summary Statistics for Last 5 Attributes}
\label{table:stats2}
\end{table}

\noindent The CV of all except attributes are smaller than 1, which means that all their data points are approximately centered, with good variability and dispersion compared to their means.\\
In addition to that, the skewness and kurtosis is helpful in describing the data distribution. All but two attributes have skew values between -0.5 and 0.5, so their data is approximately normally distributed due to a slight asymmetry. 'mpg' and 'gear' are moderately skewed with skew values between the absolute values of 0.5 and 1.\\
In addition to the skewness of data distribution, the weight in the distribution's left or right tail can also be identified by the negative and positive skew value respectively. 'cyl' and 'qsec' have more weight in the distribution's left tail, while the other remaining predictors' data distributions have more weight in the distribution's right tail (Kaufmann, 2014).. Low skewed and kurtosis will indicate a light-tailed distribution relative to a normal distribution, with little to no presence extreme values.\\
For instance, the attribute 'mpg' has a mean of 20.09, median of 19.2, and mode of 21 are relatively unequal, and in addition to its CV of 0.3 and skew value of 0.6404, it is suggested to have a moderately asymmetric and relatively heavy right-tailed data distribution (Kaufmann, 2014).\\
The kernel density estimate (KDE) plots, as visualised in Section 1.3.2 Preliminary Visualisations and Mathematical Modelling, are additionally used to provide a more complete picture of these conclusions.


\newpage
\paragraph{Correlations}
<<>>=
cor(dataset2) #correlation matrix
@
\noindent A correlation matrix lists all the Pearson product-moment correlation coefficients. The correlations will compare and describe the linear connection and relationship between pairs of attributes, through the type of correlation and its strength. A positive correlation indicates that both attribute' values will change in the same direction, while a negative correlation indicates that both attribute' values will change in opposite directions. The larger the correlation strength, the stronger the connection and relationship between that pair of attributes.\\
For instance, it is found that the 'mpg' and 'wt' predictors have the most notable linear relationship with a high correlation coefficient of -0.8725. 

\noindent Higher correlations with the dependent attribute 'mpg' of above an absolute value of 0.7 is filtered and listed.
<<echo=FALSE>>=
# check for high correlations between predictors
corrCheck = function(dataset2, threshold){
  corrMat = cor(dataset2)
  corrMat
  #find absolute correlations between the threshold and 0.95
  for (i in 1:nrow(corrMat)){
    corrs = which((abs(corrMat[i, i:ncol(corrMat)]) > threshold) & 
                    (abs(corrMat[i, i:ncol(corrMat)] < 0.95))) 
    
    #display correlations that meet the conditions
    if (length(corrs) > 0){
      lapply(corrs, FUN = function(x) (cat(paste(colnames(dataset2)[i], "and", 
                                                 colnames(dataset2)[x]), "\n"))) 
    }
  }
}
corrCheck(dataset2, 0.7) #0.7 as threshold
@

\noindent Therefore, the attributes 'dsp', 'hp', and 'wt' are chosen during this feature selection to be the independent attributes in the multiple linear regression.

\subsubsection{Preliminary Visualisations and Mathematical Modelling}

\noindent The boxplots illustrate a rough probability distribution for the attributes. The x-axis lists the attributes, while the y-axis measures the data values.\\
For instance, the mean, median and mode of 'mpg' are approximately equal, and its boxplot has symmetric halves between the first and third quartiles (Lane, n.d.). Thus, it has an approximately normal distribution.\\
\begin{center}
Boxplots of Miles in Gallons, Rear Axle Ratio, Weight in thousand Pounds (lbs), Quarter Mile Time, Displacement in Cubic Inch, and Gross Hoursepower
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
<<echo = FALSE, fig = TRUE>>= 
# boxplot
boxplot(dataset2[, c(1, 5, 6, 7)], outline=FALSE, outwex=0.2, 
        boxwex=0.5, col="mistyrose")
@
\captionof{figure}{Four Boxplots}
\label{fig:boxplot2}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
<<echo = FALSE, fig = TRUE>>= 
# boxplot
boxplot(dataset2[, 3:4], outline=FALSE, outwex=0.2, boxwex=0.5, col="mistyrose")
@
\captionof{figure}{Two Boxplots}
\label{fig:boxplot3}
\end{subfigure}
\captionof{figure}{Boxplots of Miles in Gallons, Rear Axle Ratio, Weight in thousand Pounds (lbs), Quarter Mile Time, Displacement in Cubic Inch, and Gross Horsepower}
\label{fig:box2}
\end{figure}
\end{center}

\begin{figure}[H]
\begin{center}
<<echo = FALSE, fig = TRUE>>=
# pair plot for continuous numerical data
ggpairs(dataset2, columns = c(1, 3, 4, 5, 6, 7), col="mistyrose", 
        ggplot2::aes(col = "mistyrose", alpha=9)) 
@
\captionof{figure}{Pair Plots}
\label{fig:pairplot2}
\end{center}
\end{figure}

\noindent KDE plots visualises the overall distribution through a continuous probability density curve for the numerical values in the same figure space. The y-axis is the density measurements.\\
All the overall KDE distribution curves have at the most medium kurtosis and skewness. All attributes except 'hp' and 'qsec' have bimodal or multimodel data distribution that is asymmetric, since there are two distinct peaks or also known as local maxima. 'qsec' has approximate normal data distribution as the curves are approximately symmetric, unimodal, asymptotic, and their mean, median, and mode are approximately similar (Lane, n.d.). 'hp' has an asymetric distribution that is skewed to the right.\\
The histogram density plots and their respective highest point in the curves show the patterns and trends in the numerical data. \\
The scatter plots on the lower triangle can visually identify the linear relationship and its strength, even without the correlation coefficients on the upper triangle. It can be seen that mpg and wt has the strongest linear correlation and thus relationship and association, while mpg and qsec has one of the weakest.\\
\\
\noindent For discrete categorical data, frequency barplots are used to compare each category. All categorical attributes of 'cyl','vs','am','gear', and 'carb' thus have a categorical or generalised Bernoulli or multinoulli distribution. The possible values that the attribute can take on are listed as categories in the x-axis, while the y-axis can be used to specify the frequency and the likelihood of each category. 
\begin{figure}[H]
\begin{center}
\begin{subfigure}{.48\textwidth}
<<echo = FALSE, fig = TRUE>>=
#frequencies
countCYL <- table(dataset2$cyl)
#barplot
barplot(countCYL, main="Count of Number of Cylinders", horiz=FALSE, 
        col="mistyrose", xlab = "Number of Cylinders", ylab = "Count")
@
\captionof{figure}{Barplot of Number of Cylinders}
\label{fig:bars1}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
<<echo = FALSE, fig = TRUE>>=
#frequencies
countVS <- table(dataset2$vs)
#barplot
barplot(countVS, main="Count of V/S Engine Type", horiz=FALSE, ylim = c(0, 20),
        col="mistyrose", xlab = "V/S Engine Type", ylab = "Count")
@
\captionof{figure}{Barplot of V/S Engine Type}
\label{fig:bars2}
\end{subfigure}
\captionof{figure}{Barplots of Number of Cylinders and V/S Engine Type}
\label{fig:5bars}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{subfigure}{.48\textwidth}
<<echo = FALSE, fig = TRUE>>=
#frequencies
countAM <- table(dataset2$am)
#barplot
barplot(countAM, main="Count of Automatic/Manual Transmission", horiz=FALSE, 
        col="mistyrose", xlab = "Automatic/Manual Transmission", ylab = "Count", 
        ylim = c(0, 20))
@
\captionof{figure}{Barplot of Automatic/Manual Transmission}
\label{fig:bars3}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
<<echo = FALSE, fig = TRUE>>=
#frequencies
countGear <- table(dataset2$gear)
#barplot
barplot(countGear, main="Count of Number of Forward Gears", horiz=FALSE, 
        ylim = c(0, 16), col="mistyrose", xlab = "Number of Forward Gears", 
        ylab = "Count")
@
\captionof{figure}{Barplot of Number of Forward Gears}
\label{fig:bars4}
\end{subfigure}
\captionof{figure}{Barplots of Automatic/Manual Transmission and Number of Forward Gears}
\label{fig:5bars3}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{subfigure}{.48\textwidth}
<<echo = FALSE, fig = TRUE>>=
#frequencies
countCarb <- table(dataset2$carb)
#barplot
barplot(countCarb, main="Count of Number of Carburetors", horiz=FALSE, 
        col="mistyrose", xlab = "Number of Carburetors", ylab = "Count")
@
\captionof{figure}{Barplot of Number of Carburetors}
\label{fig:bars5}
\end{subfigure}
\captionof{figure}{Barplot of Number of Carburetors}
\label{fig:5bars5}
\end{center}
\end{figure}

\noindent The five probability distributions are fitted to 'mpg' for comparisons.
\begin{figure}[H]
\begin{center}
<<echo = FALSE>>= 
# Fitting 5 probability distributions
gamma2 <- fitdist(dataset2$mpg, "gamma")
summary(gamma2)
lnorm2 <- fitdist(dataset2$mpg, "lnorm")
summary(lnorm2)
weibull2 <- fitdist(dataset2$mpg, "weibull")
summary(weibull2)
norm2 <- fitdist(dataset2$mpg, "norm")
summary(norm2)
exp2 <- fitdist(dataset2$mpg, "exp")
summary(exp2)
@
\captionof{figure}{Mathematical Modelling through the Fitting of Probability Distributions}
\label{fig:modelMath}
\end{center}
\end{figure}

\noindent Log-normal distribution is chosen, with log likelihood value of -100.773, AIC of 205.546, and BIC of 208.4774. It fits this distribution the best and is the most accurate and efficient model for the data distribution of 'mpg'. Its distribution plot comprises of a histogram and the y-axis value plotting of a best fit curve of frequency density (McNeese, 2016). The standard error values will compute the confidence interval (GraphPad Software, n.d.). \\
The distributions of the other 5 numerical predictors can also be obtained using this method of fitting distributions.

\begin{figure}[H]
\begin{center}
<<echo = FALSE, fig = TRUE>>= 
# Visualization of probability distribution
denscomp(lnorm2, addlegend = "FALSE",
         main="Histogram and Log-normal Distribution of Miles in Gallons", #title of plot
         datacol = c("mistyrose"),
         fitcol = c("gray0"),
         fitlty= c("dashed"),
         xlab="Miles in Gallons", #x axis label
         labels=TRUE, # data labels
         ylim=c(0,0.08), #y axis limit
         xlim=c(10,40)) #x axis limit
@
\captionof{figure}{Histogram and Log-normal Distribution of Miles in Gallons}
\label{fig:histMPG}
\end{center}
\end{figure}

@ predictive modelling
\subsection{Predictive Modelling}
Multiple Linear regression where 'mpg' is the dependent variable on the y-axis, and 'disp', 'hp', and 'wt' as the independent variables on the x-axises. The model summary, including coefficients, residuals, and standard error, is shown below.

<<echo=FALSE>>=
# summary of multiple linear regression model
modelMLR = lm(mpg ~ disp + hp + wt, data = dataset2)
summary(modelMLR)
@

\noindent The low t value indicates low difference relative to the data variation. This and the p-value of above 0.05 increases the evidence to reject the null hypothesis and the acceptance of the alternative that the model is significant and relevant in making estimations. The standard errors are low, thus suggesting high accuracy.\\
The model's output is a best fit equation, where 'disp', 'hp', and 'wt' can be substituted in to estimate the value of mpg. The equation is MPG = 38.0854 + 0.0043 x DISP -0.0407 x HP -4.0819 x WT.

@ model evaluation
\subsection{Model Performance Evaluations}
The difference between multiple R-squared and adjusted R-squared will measure the improvement other than expected of the model after adding a new number of predictors. The difference is large enough to be considered, which indicates that the model improved after adding the three new predictors. The rank-3 log odds of wt in estimating mpg shows a high association between it and mpg, and the estimations are accurate with minimal standard error with minimal dispersion relative to the mean value. The p-value is lower than the 0.05 threshold, which indicates the rejection of the null hypothesis and the acceptance of the alternative that wt is significant, relevant, and important in estimating mpg. wt is thus a good predictor of mpg.\\
However, hp has lower rank-2 log odds, while disp has an unacceptable rank-0 log odds. These may contribute in lowering the accuracy and reliability of the model, as they are not as strong predictors of mpg. disp should be removed from the model, due to its rank-0 log odds, low t value as in low difference relative to the data variation. This and p-value of above 0.05 increases the evidence to accept null hypothesis and rejects the alternative. This indicates that it is not significant and reliable in estimating mpg.\\
The confidence interval level is computed below.\\
<<echo=TRUE>>=
confint(modelMLR) # confidence interval of the model coefficient
@
\noindent The confidence interval levels of all attributed included are high and wide, which help to ensure good model performance in estimations. The entire data spread of the data is considered to be well-covered, with high confidence that it will contain the true mean value.
\noindent The error rate is computed below.\\
<<echo=TRUE>>=
sigma(modelMLR)/mean(dataset2$mpg) # error rate
@
\noindent The 12.667\% error rate is relatively high. This may be due to the inclusion of disp which is not useful to the model. Future improvements would be to perform more detailed feature selections and drop disp from the model in order to improve the error rate.

\subsection{Model Interpretations and Estimates}
\noindent The results indicate that the multiple linear regression model is accurate and reliable in estimating performance of cars in terms of miles per gallon. Weight of car in thousands of pounds is found to be the most important predictor of mpg, due to high linear correlation coefficient and thus strong relationship with mpg. Its rank-3 log odds, low standard error and p-value, amongst others, further supports its significance.\\
However, some improvements can be made such as to further improve the model performance. One such improvement is to remove disp predictor from the model due to meased insignificance ny performance metrics.\\
\noindent This model are now ready to be deployed to predict new value instances. To do so, a data frame is created to describe the characteristics of a number of cars based on their respective characteristics and features of the 3 predictors. These new data instances will be passed to the chosen model classifier to predict the value of 'mpg'. A similar example is demonstrated below 
<<echo=TRUE>>=
# use the multiple linear regression model to estimate data points
y_pred = predict(modelMLR, newdata = data.frame(disp = c(73, 188, 293, 303, 450), 
                                                hp = c(55, 99, 129, 199, 246),
                                                wt = c(3.5, 5.1, 2.5, 3.0, 1.6)))
y_pred
@

\noindent  The estimated values for the specified examples are assigned as continuous numerical values. For instance, a new instance with disp of 73 cubic inches, hp of 55, and wt of 3500 pounds, is estimated to have 21.8710 miles per gallon.

@ references 
\newpage
\section {References}

\expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks% save the current one
  \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
  \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
  \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
  \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
  \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
  \do\Y\do\Z\do\*\do\-\do\~\do\'\do\"\do\-}%

Analyttica Datalab (2019, February 11). \emph{Log-Likelihood- Analyttica Function Series}. Medium. \url{https://medium.com/@analyttica/log-likelihood-analyttica-function-series-cb059e0d379#:~:text=Log\%20Likelihood\%20value\%20is\%20a\%20measure\%20of\%20goodness,look\%20at\%20the\%20value\%20cannot\%20give\%20any\%20indication} \\
Fabozzi, F., Focardi, S., Rachev, S., \& Arshanapalli, B. (2014). \emph{The Basics of Financial Economics : Tools, Concepts, and Asset Management Applications}. John Wiley \& Sons, Inc. \url{https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118856406.app5.}\\
Frost, J. (2018, October 17). \emph{Difference between Descriptive and Inferential Statistics}. Statistics By Jim. \url{https://statisticsbyjim.com/basics/descriptive-inferential-statistics/}\\
GraphPad. n.d. \emph{GraphPad Prism 9 Curve Fitting Guide - Standard error of parameters}. Graphpad. \url{https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_standarad_error_of_parameters.htm}\\
Jaitley, U. (2019, February 2). \emph{Comparing Different Classification Machine Learning Models for an imbalanced dataset}. Towards Data Science. \url{https://towardsdatascience.com/comparing-differentclassification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f}\\
Kaufmann, J. (2014, September 26). \emph{What do you consider a good standard deviation?}. ResearchGate. \url{https://www.researchgate.net/post/What-do-you-consider-a-good-standard-deviation}\\
Laerd statistics. n.d. \emph{Descriptive and Inferential Statistics}. Laerd statistics.
\url{https://statistics.laerd.com/statistical-guides/descriptive-inferential-statistics.php}\\
Lane, D. n.d. \emph{Introduction to Normal Distributions}. OnlineStatBook. \url{http://onlinestatbook.com/2/normal_distribution/intro.html}\\
Taboga, M. (2017). \emph{Log-likelihood}. Statlect.com. \url{https://www.statlect.com/glossary/log-likelihood.}\\



\newpage
@ appendix
\section {Appendixes}
\subsection{Appendix A: Screen Recording of Full RSweave Process Execution}
Link to recorded video in Google Drive : \href{https://drive.google.com/file/d/1B8Fv8jvQjTLoEnExQJCO8e7gr6EqBfl5/view?usp=sharing}{<link>}

\subsection{Appendix B: Supporting R Script Code}
<<>>=
# summary statistics of the dataset
summary(dataset)

# calculate standard deviation for all attributes
sapply(dataset, sd)

# calculate variance for all attributes
sapply(dataset, var)

# find mode
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
lapply(dataset, getmode)

library(moments)

# calculate skewness
skewness(dataset)

# calculate kurtosis
kurtosis(dataset)

# calculate coefficient of variance (COV)
cv1 = 71.0720 / 121.8
cv1
cv2 = 6.7584 / 10.72
cv2
cv3 = 13.9968 / 26.26
cv3
cv4 = 1.1627 / 2.235
cv4


# summary statistics of the dataset
summary(dataset2)

# calculate standard deviation for all attributes
round(sapply(dataset2, sd), 4)

# calculate variance for all attributes
round(sapply(dataset2, var), 4)

# find mode
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
lapply(dataset2, getmode)

library(moments)

# calculate skewness
round(skewness(dataset2), 4)

# calculate kurtosis
round(kurtosis(dataset2), 4)
@

\subsection{Appendix C: Turnitin Similarity Report}
18\% similarity, not more than 3\% for each source, from Page 2 to 31. \\
Link to Turnitin Similarity Report : \href{https://drive.google.com/file/d/1cZaEBMN3NwnOnuQ8eQ7PKdfByNRA5T3e/view?usp=sharing}{<link>}

\end{document}
